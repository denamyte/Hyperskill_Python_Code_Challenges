type: code
files:
- name: main.py
  visible: true
  text: |-
    import nltk
    from nltk import word_tokenize, sent_tokenize
    import numpy as np

    def tokenize(texts):
        # tokenize all 3 sentences; do not lemmatize/stem them
        # make a list of tokens
        # append this list to a bigger list of all 3 sentences
        tokenized = []


        return tokenized

    def make_voc(tokens):
        # make your own vocabulary here
        # tokens in your vocabulary should have only lowercase
        voc = []


        return list(sorted(set(voc)))


    def make_matrix(voc, texts):
        matrix = np.zeros((len(texts), len(voc)))
        # make a binary matrix

        return matrix

    corpus = str(input()) # you get a string of 3 sentences
    texts = list(sent_tokenize(corpus)) # list of 3 strings-sentences
    tokens = tokenize(texts) # word_tokenize your sentences
    voc = make_voc(tokens) # use these tokens to make a vocabulary
    print(bag_of_words(voc, tokens)) # make a matrix
  learner_created: false
feedback_link: https://hyperskill.org/learn/step/20264#comment
status: Solved
feedback:
  message: <html>Correct solution</html>
  time: "Mon, 13 Feb 2023 23:34:51 UTC"
record: -1
