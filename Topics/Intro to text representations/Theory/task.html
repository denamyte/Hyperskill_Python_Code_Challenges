<h2>Intro to text representations</h2>
<p>In NLP, we usually convert an input text, a word, or a symbol into a numeric format to apply various mathematical operations (compare numbers, find patterns in numeric input, and so on) with them. The most convenient way is to provide a <strong>vector</strong> for each text unit. By vector, we mean an ordered sequence of numbers. In NLP, we can use them to describe any element, be it a single word or an entire text. What a vector encodes depends solely on our objective and approach. This process is called embedding (or text representation): it could be a document or word embedding.</p>
<h5 id="simple-models-for-document-embeddings">Simple models for document embeddings</h5>
<p>Let's start with the <strong>bag-of-words </strong>representation, which is the easiest one to implement. With this representation, we consider each word independently, without considering the surrounding context. We describe a text as a sequence of all words it contains, but we do not keep the original order and place. The resulting vector encodes the text and stores information about occurrences of words in it. The model is frequently used in document classification and information retrieval but also has applications in many other NLP tasks.</p>
<p>Let's look at the example. We have three reviews, each consisting of one sentence:</p>
<p>Review I: <em>easily the best album of the year.</em></p>
<p>Review II: <em>the album is amazing.</em></p>
<p>Review III: <em>loved the clean production!</em></p>
<p>First, we need to design the vocabulary and list all known words in the data. If we ignore punctuation marks, it can look as follows: "<em>easily</em>", "<em>the</em>", "<em>best</em>", "<em>album</em>", "<em>of</em>", "<em>year</em>", "<em>is</em>", "<em>amazing</em>", "<em>loved</em>", "<em>clean</em>", "<em>production</em>". The vector length should equal the vocabulary length, so it will be <code class="java">11</code> here.</p>
<p>The next step is to count all occurrences of these words in each review. For one, "<em style="font-size: inherit;">the</em>" appears twice in the first review, so we insert 2 in the corresponding cell opposite the document name. We obtain the following representations, in which a number in the vector represents the count of the related word:</p>
<pre><code class="language-no-highlight">review_1 = [1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0]                                                                                        

review_2 = [0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0]             

review_3 = [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1]</code></pre>
<p>There can be different ways of scoring. You can just point out whether a word appears in a document or not. That leads to binary vectors, with <code class="java">0</code> for each absent word and <code class="java">1</code> for each present word. Now, our representation will change a little:</p>
<pre><code class="language-no-highlight">review_1 = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]                                                                                        

review_2 = [0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0]             

review_3 = [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1]</code></pre>
<p>We create binary vectors when we are more concerned about the presence of words rather than their raw counts. The simplest sentiment analysis is an example of a task in which we can apply this representation.</p>
<p>Sometimes, when we score occurrences in the bag-of-words model, we get frequent words that are unimportant to a document we consider. Yet, there can be words with smaller scores that are relevant to the topic of our text, but the bag-of-words does not consider them. Here we have <strong>TF-IDF </strong>to help us increase proportionality and emphasize those words that contain useful information.</p>
<p><strong>TF-IDF</strong> (or term frequency-inverse document frequency) mark counts not only in a specific document but also in an entire text collection. Some words appear in each document (for example, "<em>do</em>", "<em>say</em>", "<em>have</em>"), and their scores can be high even for one text. The idea of TF-IDF is to rescale the counts to eliminate such bias. That results in a weighted bag-of-words representation.</p>
<p>To get TF-IDF, you need to calculate two metrics:</p>
<ul>
<li><strong>Term frequency </strong>captures the frequency of a word in one chosen document; it is the last method we showed in the previous section.</li>
<li><strong>Inverse document frequency </strong>measures the rarity of a word across an entire set of documents. The more common a word, the closer its weight to <code class="java">0</code>.</li>
</ul>
<p>The base of the logarithm can be different, but the natural logarithm is most commonly used. The TF-IDF score is the product of these two metrics.</p>
<h5 id="simple-models-of-word-embeddings">Simple models of word embeddings</h5>
<p>We use BOW and TF-IDF to represent a large text (a document, a novel, an article). To embed tokens, it is preferable to use Word2Vec or GloVe. What is the main difference between these two types of models? The first, document embeddings, create one vector, where each word or n-gram constitutes only one digit (there are many digits in a vector). The second, word embeddings, generate a vector, where a word is represented by the whole vector, not only one digit in the vector.</p>
<p>Word embeddings allow us to capture semantic and syntactic relations. For example, "<em>lamp</em>" and "<em>lantern</em>" will have very similar vectors, while "<em>lantern</em>" and "<em>can</em>" will have different representations.</p>
<p>We cannot simply score the values manually, as we have done with the bag-of-words and TF-IDF. Instead, the weights should be obtained during training on colossal text collections. Alternatively, we can use pre-trained embeddings provided by the <a href="https://nlp.stanford.edu/projects/glove/" rel="noopener noreferrer nofollow" target="_blank">GloVe</a> and <a href="https://fasttext.cc/docs/en/english-vectors.html" rel="noopener noreferrer nofollow" target="_blank">fastText</a> projects. The vector length can be set as a model parameter.</p>
<p>The underlying idea of <strong>Word2Vec</strong> is that the word meaning heavily depends on the context, so Word2Vec approximates the sense of a word through vectors of its surroundings so that we can note synonyms and antonyms, as well as other types of linguistic phenomena and relations (for one, a famous example is that "<em>man</em>" relates to "<em>king</em>" as "<em>woman</em>" relates to "<em>queen</em>").</p>
<p>Word2Vec has two models for producing vectors:</p>
<ul>
<li><strong>Continuous bag-of-words</strong> </li>
<li><strong>Continuous skip-gram</strong></li>
</ul>
<p>Word2Vec calculates word probabilities in a corpus and uses them to create embeddings. As a result, all words from the vocabulary get their vector representations. Similar words (that is, words appearing in similar contexts) will have similar vectors.</p>
<p>Another word embedding model, <strong>GloVe</strong>, is in many ways similar to Word2Vec. The central concept of GloVe is the simple observation that ratios of the word-word co-occurrence probabilities have the potential for encoding some form of meaning. Take, for example, the co-occurrence probabilities for target words <em>ice</em> and <em>steam</em> with various probe words from the vocabulary. Here are some actual probabilities from a 6 billion word corpus (taken from the official page of GloVe):</p>
<p><img alt="" height="131" name="image.png" src="https://ucarecdn.com/baa83f1f-acb1-46e0-ac5e-0f9211bef6da/" width="619"/></p>
<p><em>ice</em> co-occurs more frequently with <em>solid</em> than with <em>gas</em>, while <em>steam</em> co-occurs more frequently with <em>gas</em> than <em>solid</em>. Both words co-occur with their shared property <em>water</em> frequently, and both co-occur with the unrelated word <em>fashion</em> infrequently. The main task of GloVe is to learn word vectors such that their dot product equals the logarithm of the words' probability of co-occurrence.</p>
<p>For more information, refer to the <a href="https://nlp.stanford.edu/projects/glove/" rel="noopener noreferrer nofollow" target="_blank">official GloVe page</a>.</p>
<p>Another approach is <strong>one-hot encoding</strong>. It all begins with indexing all words in the sentence. For example, we have the following text: <code class="java">Tomorrow I have to go to school.</code></p>
<p>Indexing the words will look like this:</p>
<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
<tbody>
<tr>
<td>Tomorrow</td>
<td>I </td>
<td>have</td>
<td>to</td>
<td>go </td>
<td>school</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>You can notice that we do not index the same word twice. So, the word <code class="java">tomorrow</code> is in the position <code class="java">0</code>; its one-hot vector will look like this: <code class="java">[1, 0, 0, 0, 0, 0]</code></p>
<p>The length of the vector equals the number of words in the dictionary. The matrix of this sentence will be the following:</p>
<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
<tbody>
<tr>
<td> </td>
<td><strong>0</strong></td>
<td><strong>1</strong></td>
<td><strong>2</strong></td>
<td><strong>3</strong></td>
<td><strong>4</strong></td>
<td><strong>5</strong></td>
</tr>
<tr>
<td><strong>Tomorrow</strong></td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td><strong>I</strong></td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td><strong>have</strong></td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td><strong>to</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td><strong>go</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td><strong>to </strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td><strong>school</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>You could notice that the index <code class="java">3</code> occurs twice. That's because the corresponding word (<code class="java">to</code>) happens twice too.</p>
<h5 id="advanced-document-embeddings">Advanced document embeddings</h5>
<p>More complicated ways to embed a large text are the N-gram model, doc2vec, Universal Sentence Encoder, and many others.</p>
<p><strong>The N-gram model</strong> is a beneficial tool to predict the next word: this model stands behind T9, typo checking, and many other technologies. The central concept is to assign a probability to the occurrence of an N-gram or the likelihood of a word occurring next in a sequence of words and get the N-gram model. We can compare documents by assigning probabilities of the occurrence of a certain n-gram. For example, if in one document a 3-gram occurs once and in the other 52 times, these two documents will have different probabilities that this n-gram will happen in the text. Consequently, we can assume that these two documents are completely different. After computing probabilities for many n-grams, we will get a vector, where each digit is a probability number.</p>
<p><strong>Doc2Vec </strong>model is very similar to Word2Vec. It uses Word2Vec as its basis. In doc2vec, we use word2vec to compute a vector for each word. We get an <em>n</em> vector (the number of words in the document), then we add another vector responsible for the document identification. You can read how to use Doc2Vec in Gensim in <a href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener noreferrer nofollow" target="_blank">its official documentation</a>.</p>
<p><strong>Universal Sentence Encoder</strong> embeds each sentence and then semantically compares the sentences. The simple model embeds sentences using Word2Vec technology, and we get sentence vectors. Then, the <a href="https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark" rel="noopener noreferrer nofollow" target="_blank">STS Benchmark</a> (a dataset) evaluates how similarity scores computed using sentence embeddings align with human judgments. The benchmark requires systems to return similarity scores for a diverse selection of sentence pairs. Pearson correlation is then used to evaluate the quality of the machine similarity scores against human judgments. You can read more about using USE in Python in the <a href="https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder" rel="noopener noreferrer nofollow" target="_blank">Universal Sentence Encoder</a> article on TensorFlow.</p>
<h5 id="contextualized-word-embeddings">Contextualized word embeddings</h5>
<p>Word2Vec ignores the nearby context. The same applies to GloVe. What does this mean? Some words have two or more meanings, which may vary. We will always have one vector if we pre-train our model without paying attention to the context. This causes many problems, especially if we analyze a word with a lot of meanings like <code class="java">stick</code>. The same thing in GloVe is that the word will always have a vector <code class="java">[0.1, 200, 0.3, -4,1, -0.0007, 100, 101, 3]</code>, whether it is a verb or a noun.</p>
<p>That's why we have <strong>contextualized word embeddings</strong>. An excellent example of a contextualized model is ELMo embeddings. They are based on the bi-directional LSTM model. ELMo processes the whole sentence before vectorizing each of the words inside it. Models that do not pay attention to the context are called non-contextualized word embeddings.</p>
<p>Other contextualized word embeddings are Open AI Transformer, Google's BERT, and ULM-Fit.</p>
<p>OpenAI was the first transformer-based language model with fine-tuning. To be more precise, it uses only the transformer's decoder. It is uni-directional language modeling.</p>
<p>Bidirectional Encoder Representations from Transformer (BERT) is bidirectional transformer-based masked language modeling. There are two default versions of this model: large and base. You can download the first on <a href="https://huggingface.co/bert-large-uncased" rel="noopener noreferrer nofollow" target="_blank">Hugging Face</a>.</p>
<p>Universal Language Model Fine-tuning for text classification (ULM-Fit) is another language <a href="https://arxiv.org/abs/1801.06146" rel="noopener noreferrer nofollow" target="_blank">model</a> proposed in 2018. ULM-Fit first trains long memory (LM) on an enormous general domain corpus with a bidirectional long memory. Then, it tunes LM on target task data. In the end, it adjusts as a classifier on the target task.</p>
<h5 id="conclusion">Conclusion</h5>
<p>We have discussed three types of embeddings: simple and advanced document embeddings and simple word embeddings. In the first group, we have bag-of-words and TF-IDF. We introduced you to Doc2Vec and Universal Sentence Encoder, as well as the concept of the N-gram model with complicated document embeddings. We have also discussed Word2Vec and GloVe.</p>
<p>Finally, we discussed what contextualized word embeddings are on the example of three example models.</p>
<p>Let's see what are the pros and cons of each system:</p>
<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
<tbody>
<tr>
<td> </td>
<td>Pros</td>
<td>Cons</td>
</tr>
<tr>
<td>TF-IDF                        </td>
<td>Highly intuitive; easily computable</td>
<td>Requires very high memory when there is a large dataset             </td>
</tr>
<tr>
<td>BOW</td>
<td>Highly intuitive; easily computable; very efficient to classify texts</td>
<td>New words increase the vocabulary, so the computation time increases too</td>
</tr>
<tr>
<td>W2V</td>
<td>Highly intuitive; processes the context</td>
<td>Difficult to train</td>
</tr>
<tr>
<td>D2V</td>
<td>Highly intuitive; processes the context</td>
<td>Difficult to train</td>
</tr>
<tr>
<td>BERT</td>
<td>Extraordinary results</td>
<td>Difficult to train</td>
</tr>
<tr>
<td>One-hot </td>
<td>Highly intuitive and straightforward to compute</td>
<td>Requires very high memory when there is a large vocabulary. Word similarity context is not defined</td>
</tr>
<tr>
<td>N-gram </td>
<td>Easy to compute</td>
<td>Modest results</td>
</tr>
</tbody>
</table>
<p>It's time to put what we learned to practice!</p>
